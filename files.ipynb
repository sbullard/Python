{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files in Python\n",
    "---\n",
    "\n",
    "**Handiling files in Python is done in a very similar manner between file types. However, there are some gotchas that do occur. This notebook delves into 5 of the more common file types**\n",
    "\n",
    "## Index\n",
    "1. Text Files\n",
    "2. CSV & Excel Files \n",
    "3. JSON Files\n",
    "4. PDF Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Files\n",
    "---\n",
    " \n",
    "* Create or open existing text file using the built-in open() function\n",
    "* open() returns a file object and takes 2 args open('filename', mode)\n",
    "\n",
    "**There are a number of modes that can be used for the 'mode' argument in open(), the most common are listed here:**\n",
    "\n",
    "* r = read only\n",
    "* w = write only\n",
    "* a = append\n",
    "* r+ or w+ = read and write\n",
    "* b = binary mode (for non-text files)\n",
    "* modes can be combined, rb = read binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_io.TextIOWrapper'>\n",
      "\n",
      "<_io.TextIOWrapper name='data/text1.txt' mode='r' encoding='cp1252'>\n"
     ]
    }
   ],
   "source": [
    "# Basic method for opening an already existing file\n",
    "txtfile = open('data/text1.txt')\n",
    "print(type(txtfile))\n",
    "print()\n",
    "\n",
    "# When looking at whats in the txtfile variable, note that txtfile is a \n",
    "# wrapper to the text1.txt file and by default opens in read only mode ('r')\n",
    "# Note: ALL files open in read only mode by default. \n",
    "print(txtfile)\n",
    "\n",
    "# Files are closed manually with file.close() like so:\n",
    "txtfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Important***\n",
    "**While the above method for opening a file works, using the with keyword when dealing with file objects allows for automatic file closing after use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the files tutorial.\n",
      "First will see how Python can be used to work with text files.\n",
      "\n",
      "nothing returned here:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here the text1.txt file is opened using a with stamement and is stored in\n",
    "# the f variable\n",
    "\n",
    "# The file is then read using the built-in read() method which only reads\n",
    "# the file once and will return an empty string if called again\n",
    "\n",
    "with open('data/text1.txt') as f:\n",
    "    print(f.read())\n",
    "    print()\n",
    "    print('nothing returned here:')\n",
    "    print(f.read())\n",
    "    \n",
    "# Also note close() not required due to using with statement to open file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial f.readlines() stored in var returned here:\n",
      "['Welcome to the files tutorial.\\n', 'First will see how Python can be used to work with text files.']\n",
      "\n",
      "Nothing returned here as f.readlines() already used:\n",
      "[]\n",
      "\n",
      "Var can be re-used:\n",
      "['Welcome to the files tutorial.\\n', 'First will see how Python can be used to work with text files.']\n",
      "\n",
      "Here first word from each line returned:\n",
      "Welcome\n",
      "First\n"
     ]
    }
   ],
   "source": [
    "# Text files can also be read line-by-line using the readlines() built-in \n",
    "# function, note that each line is stored in a list index, and just like\n",
    "# read() function the file is only read through once\n",
    "\n",
    "# Note that here the text is stored in a list so that it can be re-read\n",
    "txtlist = []\n",
    "\n",
    "with open('data/text1.txt') as f:\n",
    "    txtlist = f.readlines()\n",
    "    \n",
    "    print('Initial f.readlines() stored in var returned here:')\n",
    "    print(txtlist)\n",
    "    print()\n",
    "    \n",
    "    print('Nothing returned here as f.readlines() already used:')\n",
    "    print(f.readlines())\n",
    "    \n",
    "    print()\n",
    "    print('Var can be re-used:')\n",
    "    print(txtlist)\n",
    "\n",
    "\n",
    "print() \n",
    "print('Here first word from each line returned:')\n",
    "for line in txtlist:\n",
    "    # Split takes each line and puts it into its own list, so here each line\n",
    "    # is split and the first word in each list is pulled (index[0])\n",
    "    print(line.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is written text\n",
      "s is written text\n"
     ]
    }
   ],
   "source": [
    "# In order to write to a file, the mode w must be set\n",
    "txtwrite = open('data/text2.txt', 'w+')\n",
    "txtwrite.write('This is written text')\n",
    "\n",
    "# The funtion seek() is used to return the text cursor to a certain point in\n",
    "# a file, here 0 is used which goes to the start of the file. if 3 is used,\n",
    "# it will go the third char in the txt. \n",
    "txtwrite.seek(0)\n",
    "print(txtwrite.read())\n",
    "\n",
    "txtwrite.seek(3)\n",
    "print(txtwrite.read())\n",
    "\n",
    "txtwrite.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is written text\n",
      "Appended line....yeah\n"
     ]
    }
   ],
   "source": [
    "# Writing to exsitng files will erase all data, using the append ('a') mode\n",
    "# will prevent this\n",
    "\n",
    "# Also note that write() is used to append to a file and not append() function\n",
    "# which is used to add items to a list\n",
    "\n",
    "txtappend = open('data/text2.txt', 'a+')\n",
    "txtappend.write('\\nAppended line....yeah')\n",
    "txtappend.seek(0)\n",
    "print(txtappend.read())\n",
    "txtappend.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CSV & Excel Files\n",
    "---\n",
    "CSV Files are comma delimited and tabular in nature similar to Excel, however they do not save or conatin any formulas or calculations like excel does, so they are really just good for tabular data storage.\n",
    "\n",
    "Cells in both CSV and Excel files are tabular in nature (row * column based) and because of this CSV and Excel data are usually numerical in \n",
    "nature and used for some sort of analytical purpose.\n",
    "\n",
    "### There are two ways to work with CSV &  Excel files:\n",
    "1. Using the csv built-in module (csv files only)\n",
    "2. Using External packages such as Numpy or Pandas (both csv and excel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in csv module\n",
    "The simplest method for dealing with CSV files is the built-in CSV module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file data read in by row:\n",
      "----------------------------------------\n",
      "['date,changePercent,close,high,low,open,volume']\n",
      "['2019-03-05,0.13699999999999998,29.2,29.31,28.76,29.07,1890970']\n",
      "['2019-03-06,-3.253,28.25,29.155,28.14,29.06,2466049']\n",
      "['2019-03-07,-3.15,27.36,28.3,27.31,28.16,3102396']\n",
      "['2019-03-08,-1.974,26.82,27.16,26.51,26.93,2864338']\n",
      "['2019-03-11,2.61,27.52,27.57,26.73,26.76,3497718']\n",
      "['2019-03-12,3.343,28.44,28.59,27.6,27.73,3112615']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Here the file is read using csv.reader() and printed out, \n",
    "# however note that that each row is given it's own list\n",
    "print('CSV file data read in by row:')\n",
    "print('----------------------------------------')\n",
    "with open('data/AA_daily.csv') as csv_file:\n",
    "    data = csv.reader(csv_file, delimiter=' ')\n",
    "    for row in data:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Only returned Date and close price:\n",
      "----------------------------------------\n",
      "2019-03-05 29.2\n",
      "2019-03-06 28.25\n",
      "2019-03-07 27.36\n",
      "2019-03-08 26.82\n",
      "2019-03-11 27.52\n",
      "2019-03-12 28.44\n"
     ]
    }
   ],
   "source": [
    "# Using csv module DictReader function, data can be stored with\n",
    "# each pertinent row value in a key/value dictionary\n",
    "# Note that each row is a separate dictionary, to see this simply print row\n",
    "print()\n",
    "print('Only returned Date and close price:')\n",
    "print('----------------------------------------')\n",
    "with open('data/AA_daily.csv') as csv_file:\n",
    "    data = csv.DictReader(csv_file)\n",
    "    for row in data:\n",
    "        print(row['date'], row['close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Writing to CSV file is different than reading as the column headers \n",
    "# here called fieldnames must be included. The func DictWriter() is used \n",
    "# to get the file, the func writeheader() is used to populate fieldnames\n",
    "# the func writerow() writes the row, note in dict key/val format\n",
    "with open('data/test_csv.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['first_name', 'last_name']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow({'first_name': 'Jack', 'last_name':'Splatt'})\n",
    "    writer.writerow({'first_name': 'Paul', 'last_name':'Bunyon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack Splatt\n",
      "Paul Bunyon\n"
     ]
    }
   ],
   "source": [
    "# This reads the file from the above example\n",
    "with open('data/test_csv.csv') as csv_file:\n",
    "    data = csv.DictReader(csv_file)\n",
    "    for row in data:\n",
    "        print(row['first_name'], row['last_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using external packages with csv and excel files\n",
    "\n",
    "One of the more simplistic methods for getting data from CSV files to use is pandas and/or numpy to read the data (see pandas and numpy notebooks for detailed info on each). The csv module is faster than this methodology overall, but \n",
    "for smaller data sets, this really shouldn't be an issue. \n",
    "\n",
    "There are 2 methods for getting CSV data into numpy and 1 for pandas: \n",
    "* loadtxt (numpy) \n",
    "* genfromtxt (numpy)\n",
    "* read_csv (pandas)\n",
    "\n",
    "I performed a test to see which method was fastest, and the answer was genfromtxt. However, numpy is best for large numercial calcualtions and doesn't do text/headers very well. \n",
    "\n",
    "For general cases (not huge datasets) pandas will be the way to go as it\n",
    "has a read/write methodology for all file types. So from here on out I only\n",
    "use pandas.\n",
    "\n",
    "Final note, usually data read into pandas is used for some type of analysation, therefore appending to the original data will be rare, usually\n",
    "one will perform some kind of calculations or add columns with new\n",
    "calculated values to the original table rather than add new rows \n",
    "to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV table as read in by Pandas\n",
      "----------------------------------------\n",
      "  first_name last_name\n",
      "0       Jack    Splatt\n",
      "1       Paul    Bunyon\n",
      "\n",
      "CSV table appended to and re-read in by Pandas\n",
      "----------------------------------------\n",
      "  first_name last_name\n",
      "0       Jack    Splatt\n",
      "1       Paul    Bunyon\n",
      "2       Indy     Jones\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reading data in pandas is really easy using read_csv()\n",
    "df = pd.read_csv('data/test_csv.csv', delimiter=\",\")\n",
    "print('CSV table as read in by Pandas')\n",
    "print('----------------------------------------')\n",
    "print(df)\n",
    "\n",
    "# Appending Data to the pands dataframe, note ignore-index=True must be used\n",
    "df = df.append({'first_name':'Indy', 'last_name':'Jones'}, ignore_index=True)\n",
    "\n",
    "# Write new table data to orginial CSV using to_csv()\n",
    "# note that index=False\n",
    "df.to_csv('data/test_csv2.csv', index=False)\n",
    "\n",
    "# re-reading file after appending, note to re-write the entire file, run the\n",
    "# cell above\n",
    "df = pd.read_csv('data/test_csv2.csv', delimiter=\",\")\n",
    "print()\n",
    "\n",
    "print('CSV table appended to and re-read in by Pandas')\n",
    "print('----------------------------------------')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting between file types using pandas\n",
    "\n",
    "Pandas is also good for converting data between file types as both Excel and CSV files are handled similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   first_name last_name\n",
      "id                     \n",
      "0        Jack    Splatt\n",
      "1        Paul    Bunyon\n"
     ]
    }
   ],
   "source": [
    "# Convert CSV file to Excel using pandas to_excel() function\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/test_csv.csv', delimiter=',')\n",
    "df.to_excel('data/test_excel.xlsx', index=False)\n",
    "\n",
    "df = pd.read_excel('data/test_excel.xlsx')\n",
    "df.index.name = 'id'\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. JSON Files\n",
    "---\n",
    "JSON (JavaScript Object Notation) is a key/value syntx for storing and exchanging data. JSON data is formatted in similar fashion to python data types, however there are a couple of exceptions\n",
    "* true and false are not capitalized in JSON\n",
    "* and None = null in JSON\n",
    "\n",
    "JSON uses a key/value syntax (similar to python dictionaries). Listed below are all of the datatypes and some key rules:\n",
    "* keys are ALWAYS strings\n",
    "* values can be nums, true/false, lists, null, or dictionaries\n",
    "* note that all strings must use double quotes \" \" in JSON\n",
    "\n",
    "Python has a built-in JSON module to handle JSON files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JSONDecodeError', 'JSONDecoder', 'JSONEncoder', '__all__', '__author__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_default_decoder', '_default_encoder', 'codecs', 'decoder', 'detect_encoding', 'dump', 'dumps', 'encoder', 'load', 'loads', 'scanner']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Below are the methods available in the json module \n",
    "print(dir(json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "\n",
      "{'title': 'Gattica', 'release_year': 1997, 'is_awesome': True, 'won_oscar': False, 'actors': ['Ethan Hawke', 'Uma Thurman', 'Alan Arkin', 'Loren Dean'], 'budget': None, 'credits': {'director': 'Andrew Niccol', 'writer': 'Andrew Nicool', 'composer': 'Michale Nyman', 'cinematographer': 'Slawomir Idziak'}}\n",
      "\n",
      "Gattica\n",
      "\n",
      "['Ethan Hawke', 'Uma Thurman', 'Alan Arkin', 'Loren Dean']\n",
      "\n",
      "1997\n"
     ]
    }
   ],
   "source": [
    "# json.load() reads json file data into a python dictionary\n",
    "# Note that after json converted to dict, True and False are capitalized\n",
    "\n",
    "json_file = open('data/json_data.json', 'r', encoding='utf-8' )\n",
    "movie = json.load(json_file)\n",
    "json_file.close()\n",
    "\n",
    "print(type(movie))\n",
    "print()\n",
    "print(movie)\n",
    "print()\n",
    "print(movie['title'])\n",
    "print()\n",
    "print(movie['actors'])\n",
    "print()\n",
    "print(movie['release_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "\n",
      "{'title': 'Tron Legacy', 'composer': 'Daft Punk', 'year': 2010, 'budget': 170000000, 'actors': None, 'won_oscar': False}\n",
      "\n",
      "Tron Legacy\n"
     ]
    }
   ],
   "source": [
    "# JSON data is usually consumed through an api rather than a file. Api data\n",
    "# is usally read in as a string of data\n",
    "# json.loads() function can load JSON from a string of data into a python dict\n",
    "\n",
    "sample_api = '''\n",
    "    {\"title\" : \"Tron Legacy\",\n",
    "     \"composer\" : \"Daft Punk\",\n",
    "     \"year\" : 2010,\n",
    "     \"budget\" : 170000000,\n",
    "     \"actors\" : null,\n",
    "     \"won_oscar\" : false  \n",
    "    }\n",
    "    '''\n",
    "\n",
    "tron = json.loads(sample_api)\n",
    "print(type(tron))\n",
    "print()\n",
    "print(tron)\n",
    "print()\n",
    "print(tron['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "\n",
      "{\"title\": \"Gattica\", \"release_year\": 1997, \"is_awesome\": true, \"won_oscar\": false, \"actors\": [\"Ethan Hawke\", \"Uma Thurman\", \"Alan Arkin\", \"Loren Dean\"], \"budget\": null, \"credits\": {\"director\": \"Andrew Niccol\", \"writer\": \"Andrew Nicool\", \"composer\": \"Michale Nyman\", \"cinematographer\": \"Slawomir Idziak\"}}\n"
     ]
    }
   ],
   "source": [
    "# json.dumps() converts a python dictionary into a valid JSON string\n",
    "# Note that true and false have been converted to lowercase\n",
    "\n",
    "print(type(json.dumps(movie)))\n",
    "print()\n",
    "print(json.dumps(movie))\n",
    "\n",
    "# If some non-ASCII character is used in conversion\n",
    "# use json.dumps(movie, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dump() writes JSON object to file \n",
    "# A dictionary is created and then converted to a JSON file\n",
    "movie2 = {}\n",
    "movie2[\"title\"] = \"Minority Report\"\n",
    "movie2[\"director\"] = \"Steven Speilberg\"\n",
    "movie2[\"composer\"] = \"John Williams\"\n",
    "movie2[\"actors\"] = [\"Tom Cruise\", \"Colin Farrell\", \"Samantha Morton\"]\n",
    "movie2[\"is_awesome\"] = True\n",
    "movie2[\"cinematographer\"] = \"Janusz Kami\\u0144ski\" \n",
    "\n",
    "write_file = open('data/json_data2.json', 'w', encoding='utf-8')\n",
    "\n",
    "json.dump(movie2, write_file, ensure_ascii=False)\n",
    "\n",
    "write_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     origin destination  duration\n",
      "0  New York       Paris       435\n",
      "1    Moscow       Paris       235\n",
      "2      Lima    New York       455\n"
     ]
    }
   ],
   "source": [
    "# JSON file load with pandas\n",
    "import pandas as pd\n",
    "\n",
    "get_data = pd.read_json(f'data/flights.json')\n",
    "print(get_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# JSON file creation with pandas\n",
    "print(type(get_data))\n",
    "get_data.to_json(f'data/flights2.json', orient='records')\n",
    "\n",
    "# If orientation is not used, JSON format will be off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PDF Files\n",
    "---\n",
    "Python does not nativally handle PDF files so two external packages are used this notebook.\n",
    "\n",
    "* PyPDF2 - converts simple, text based PDF files into python text format\n",
    "* NLTK - is used extensively in the AI field of Natural Language Processing\n",
    "\n",
    "For more on NLTK, see my Natural Language processing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages in pdf:\n",
      "242\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# nltk is used here to clean and convert phrases into keywords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Note that 'rb' is used for pdf files as they are in binary format, \n",
    "# 'rb' stands for read binary.\n",
    "mypdf = open('data/pdf1.pdf', mode='rb')\n",
    "\n",
    "# Once file is opened, the funtion PdfFileReader() is used to extract the text\n",
    "pdf_text = PyPDF2.PdfFileReader(mypdf)\n",
    "\n",
    "# The numPages attribute gets the number pages\n",
    "num_pages = pdf_text.numPages\n",
    "print('Number of pages in pdf:')\n",
    "print(num_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  List of Exhibits\n",
      "    xi\n",
      "Exhibit 16.3  Cash Flow from Operating \n",
      " t-Making) Activities in \n",
      "Decline Scenario 127Exhibit 17.1  Three Financial Statements and \n",
      "Footnotes 134\n",
      "Exhibit 18.1  External Financial Statements \n",
      "of Business (without Footnotes) 146\n",
      "Exhibit 19.1 External Income Statement for Year 160\n",
      "Exhibit 19.2  t Report for Year 163\n",
      "\n",
      "Exhibit 19.3  5% Sales Prices versus 5% Sales \n",
      "Volume Increase \n",
      "165Exhibit 19.4  EBIT Breakeven Point for Lower \n",
      "Sales Prices and Lower Sales Volume 167\n",
      "fbetw.indd   xifbetw.indd   xi23-11-2013   16:31:19\n",
      "23-11-2013   16:31:19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To extract text from a specific page, use getPage(pagenum) and extractText()\n",
    "# note that this pdf is image based, therefore extractText() must be used,\n",
    "# and even so the text comes back formatted strangely many times\n",
    "page = pdf_text.getPage(12)\n",
    "pagetxt = page.extractText()\n",
    "print(pagetxt)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Tokens, Not Cleaned so a ton of crap included\n",
      "---------------------------------------------------------------------\n",
      "['List', 'of', 'Exhibits', 'xi', 'Exhibit', '16.3', 'Cash', 'Flow', 'from', 'Operating', 't-Making', ')', 'Activities', 'in', 'Decline', 'Scenario', '127Exhibit', '17.1', 'Three', 'Financial', 'Statements', 'and', 'Footnotes', '134', 'Exhibit', '18.1', 'External', 'Financial', 'Statements', 'of', 'Business', '(', 'without', 'Footnotes', ')', '146', 'Exhibit', '19.1', 'External', 'Income', 'Statement', 'for', 'Year', '160', 'Exhibit', '19.2', 't', 'Report', 'for', 'Year', '163', 'Exhibit', '19.3', '5', '%', 'Sales', 'Prices', 'versus', '5', '%', 'Sales', 'Volume', 'Increase', '165Exhibit', '19.4', 'EBIT', 'Breakeven', 'Point', 'for', 'Lower', 'Sales', 'Prices', 'and', 'Lower', 'Sales', 'Volume', '167', 'fbetw.indd', 'xifbetw.indd', 'xi23-11-2013', '16:31:19', '23-11-2013', '16:31:19']\n"
     ]
    }
   ],
   "source": [
    "# Because the initial read-in text returns timestamps and all kinds of\n",
    "# non-words it mus be cleaned. \n",
    "\n",
    "# The nltk package  is used to convert text to a list of tokens in order \n",
    "# to clean up the non-word issue.\n",
    "tokens = nltk.word_tokenize(pagetxt)\n",
    "print('Initial Tokens, Not Cleaned so a ton of crap included')\n",
    "print('---------------------------------------------------------------------')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaner List, but some punctations and numbers snuck through\n",
      "---------------------------------------------------------------------\n",
      "['List', 'of', 'Exhibits', 'xi', 'Exhibit', 'Cash', 'Flow', 'from', 'Operating', 't-Making', ')', 'Activities', 'in', 'Decline', 'Scenario', 'Three', 'Financial', 'Statements', 'and', 'Footnotes', 'Exhibit', 'External', 'Financial', 'Statements', 'of', 'Business', '(', 'without', 'Footnotes', ')', 'Exhibit', 'External', 'Income', 'Statement', 'for', 'Year', 'Exhibit', 't', 'Report', 'for', 'Year', 'Exhibit', '%', 'Sales', 'Prices', 'versus', '%', 'Sales', 'Volume', 'Increase', 'EBIT', 'Breakeven', 'Point', 'for', 'Lower', 'Sales', 'Prices', 'and', 'Lower', 'Sales', 'Volume']\n",
      "\n",
      "Super Clean List\n",
      "---------------------------------------------------------------------\n",
      "['List', 'Exhibits', 'xi', 'Exhibit', 'Cash', 'Flow', 'Operating', 't-Making', 'Activities', 'Decline', 'Scenario', 'Three', 'Financial', 'Statements', 'Footnotes', 'Exhibit', 'External', 'Financial', 'Statements', 'Business', 'without', 'Footnotes', 'Exhibit', 'External', 'Income', 'Statement', 'Year', 'Exhibit', 'Report', 'Year', 'Exhibit', 'Sales', 'Prices', 'versus', 'Sales', 'Volume', 'Increase', 'EBIT', 'Breakeven', 'Point', 'Lower', 'Sales', 'Prices', 'Lower', 'Sales', 'Volume']\n"
     ]
    }
   ],
   "source": [
    "# We can see above that nltk split all sections approppriately, now cleaning\n",
    "# begin, note in this example, I do so using pure python. However, the next\n",
    "# section delves into much simpler ways to handle the problem\n",
    "\n",
    "clean_list = []\n",
    "\n",
    "for token in tokens:\n",
    "    # Don't add to clean list if last 4 letters are indd \n",
    "    # or if last value is a digit\n",
    "    if token[-4:] == 'indd' or token[-1].isdigit() or token[0:1].isdigit():\n",
    "        continue\n",
    "    # Remove first char if number and second char is not digit\n",
    "    elif token[0].isdigit() and not token[1].isdigit():\n",
    "        token = token[1:]\n",
    "        clean_list.append(token)\n",
    "    # Token is a word, so simply append\n",
    "    else:\n",
    "        clean_list.append(token)\n",
    "\n",
    "print()\n",
    "print('Cleaner List, but some punctations and numbers snuck through')\n",
    "print('---------------------------------------------------------------------')\n",
    "print(clean_list)\n",
    "\n",
    "# Note the clean list also has problems even after my initial scrub\n",
    "# further cleaning can be done by removing punctuations like:\n",
    "punctuations = ['(',')',';',':','[',']',',', '%']\n",
    "\n",
    "# Simple words like 'I' 'and', 'the', can also be removed using nltk stopwords\n",
    "# note that english stopwords is just a list of words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# To further clean the list to remove all the junk, a final list can be \n",
    "# generated by eliminating all of the words from punctuations and stop_words\n",
    "super_clean_list = [word for word in clean_list \n",
    "                    if not word in stop_words and not word in punctuations]\n",
    "print()\n",
    "print('Super Clean List')\n",
    "print('---------------------------------------------------------------------')\n",
    "print(super_clean_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thr\n",
      " nancial statements\n",
      "    17\n",
      " \n",
      "Interest Expense:\n",
      " The amount of interest on debt (interest-bearing liabilities) for the period.\n",
      " nancing charges may also be included, such as loan origination fees.\n",
      " \n",
      "Income Tax Expense:\n",
      " The total amount due the government (both federal and state) on the amount of taxable income of the business during the period. Taxable income is multiplied by the \n",
      "appropriate tax rates. The income tax expense does not include \n",
      "\n",
      "other types of taxes, such as unemployment and Social Security \n",
      "\n",
      "taxes on the company’s payroll. These other, nonincome taxes \n",
      "\n",
      "are included in operating expenses.c02.indd   17c02.indd   1712-12-2013   15:32:17\n",
      "12-12-2013   15:32:17\n"
     ]
    }
   ],
   "source": [
    "# Note that using the above approach to get the text into tokens is\n",
    "# a cumbersome task. Using Natural Language Processing libraries such as\n",
    "# spaCy, cleaning and preparing text for further analysis is easy. By default\n",
    "# spacy documents create tokens automatically behind the scenes and removing\n",
    "# unnecessary text is really simple using regular expressions\n",
    "\n",
    "# Below is the page text pulled from the pdf in its raw form:\n",
    "import PyPDF2\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "mypdf = open('data/pdf1.pdf', mode='rb')\n",
    "pdf_text = PyPDF2.PdfFileReader(mypdf)\n",
    "page = pdf_text.getPage(32)\n",
    "pagetxt = page.extractText()\n",
    "print(pagetxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thr\n",
      " nancial statements\n",
      "    17\n",
      " \n",
      "interest expense:\n",
      " the amount of interest on debt (interest-bearing liabilities) for the period.\n",
      " nancing charges may also be included, such as loan origination fees.\n",
      " \n",
      "income tax expense:\n",
      " the total amount due the government (both federal and state) on the amount of taxable income of the business during the period. taxable income is multiplied by the \n",
      "appropriate tax rates. the income tax expense does not include \n",
      "\n",
      "other types of taxes, such as unemployment and social security \n",
      "\n",
      "taxes on the company’s payroll. these other, nonincome taxes \n",
      "\n",
      "are included in operating expenses.c02.indd   17c02.indd   1712-12-2013   15:32:17\n",
      "12-12-2013   15:32:17\n"
     ]
    }
   ],
   "source": [
    "# Cleaning text for processing using regex\n",
    "\n",
    "# 1. Make all letters lowercase\n",
    "processed_pdf = pagetxt.lower()\n",
    "print(processed_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thr  nancial statements          interest expense   the amount of interest on debt  interest bearing liabilities  for the period   nancing charges may also be included  such as loan origination fees    income tax expense   the total amount due the government  both federal and state  on the amount of taxable income of the business during the period  taxable income is multiplied by the  appropriate tax rates  the income tax expense does not include   other types of taxes  such as unemployment and social security   taxes on the company s payroll  these other  nonincome taxes   are included in operating expenses c   indd     c   indd                                                \n"
     ]
    }
   ],
   "source": [
    "# 2. Remove all chars that aren't letters\n",
    "processed_pdf = re.sub('[^a-zA-Z]', ' ', processed_pdf)\n",
    "print(processed_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thr nancial statements interest expense the amount of interest on debt interest bearing liabilities for the period nancing charges may also be included such as loan origination fees income tax expense the total amount due the government both federal and state on the amount of taxable income of the business during the period taxable income is multiplied by the appropriate tax rates the income tax expense does not include other types of taxes such as unemployment and social security taxes on the company s payroll these other nonincome taxes are included in operating expenses c indd c indd \n"
     ]
    }
   ],
   "source": [
    "# 3. Remove all white space\n",
    "processed_pdf = re.sub(r'\\s+', ' ', processed_pdf)\n",
    "print(processed_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thr\n",
      "nancial\n",
      "statements\n",
      "interest\n",
      "expense\n",
      "the\n",
      "amount\n",
      "of\n",
      "interest\n",
      "on\n",
      "debt\n",
      "interest\n",
      "bearing\n",
      "liabilities\n",
      "for\n",
      "the\n",
      "period\n",
      "nancing\n",
      "charges\n",
      "may\n",
      "also\n",
      "be\n",
      "included\n",
      "such\n",
      "as\n",
      "loan\n",
      "origination\n",
      "fees\n",
      "income\n",
      "tax\n",
      "expense\n",
      "the\n",
      "total\n",
      "amount\n",
      "due\n",
      "the\n",
      "government\n",
      "both\n",
      "federal\n",
      "and\n",
      "state\n",
      "on\n",
      "the\n",
      "amount\n",
      "of\n",
      "taxable\n",
      "income\n",
      "of\n",
      "the\n",
      "business\n",
      "during\n",
      "the\n",
      "period\n",
      "taxable\n",
      "income\n",
      "is\n",
      "multiplied\n",
      "by\n",
      "the\n",
      "appropriate\n",
      "tax\n",
      "rates\n",
      "the\n",
      "income\n",
      "tax\n",
      "expense\n",
      "does\n",
      "not\n",
      "include\n",
      "other\n",
      "types\n",
      "of\n",
      "taxes\n",
      "such\n",
      "as\n",
      "unemployment\n",
      "and\n",
      "social\n",
      "security\n",
      "taxes\n",
      "on\n",
      "the\n",
      "company\n",
      "s\n",
      "payroll\n",
      "these\n",
      "other\n",
      "nonincome\n",
      "taxes\n",
      "are\n",
      "included\n",
      "in\n",
      "operating\n",
      "expenses\n",
      "c\n",
      "indd\n",
      "c\n",
      "indd\n"
     ]
    }
   ],
   "source": [
    "# At this point the above text is readable, but some of the words are \n",
    "# incomplete and there is nonsense text like 'indd' and 'c' at the end.\n",
    "# Such words are useless for any type of keyword/token analysis.\n",
    "# So to fix this, the python library spacy can be used\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# First a spacy document must be created, doing so automatically creates\n",
    "# tokens for each word and sentence in the document.\n",
    "\n",
    "spacy_doc = nlp(processed_pdf)\n",
    "for token in spacy_doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thr\n",
      "nancial\n",
      "statements\n",
      "interest\n",
      "expense\n",
      "interest\n",
      "debt\n",
      "interest\n",
      "bearing\n",
      "liabilities\n",
      "period\n",
      "nancing\n",
      "charges\n",
      "included\n",
      "loan\n",
      "origination\n",
      "fees\n",
      "income\n",
      "tax\n",
      "expense\n",
      "total\n",
      "government\n",
      "federal\n",
      "state\n",
      "taxable\n",
      "income\n",
      "business\n",
      "period\n",
      "taxable\n",
      "income\n",
      "multiplied\n",
      "appropriate\n",
      "tax\n",
      "rates\n",
      "income\n",
      "tax\n",
      "expense\n",
      "include\n",
      "types\n",
      "taxes\n",
      "unemployment\n",
      "social\n",
      "security\n",
      "taxes\n",
      "company\n",
      "s\n",
      "payroll\n",
      "nonincome\n",
      "taxes\n",
      "included\n",
      "operating\n",
      "expenses\n",
      "c\n",
      "indd\n",
      "c\n",
      "indd\n"
     ]
    }
   ],
   "source": [
    "# Notice there are a lot of single letters and nonsense words suc as indd,\n",
    "# as well as stop-words such as 'the', 'of', 'and', etc.\n",
    "# spacey can remove these using the is_stop attribute\n",
    "\n",
    "for token in spacy_doc:\n",
    "    if not token.is_stop:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the keyword list is smaller, however, there are still single letters as\n",
    "# well as two misspelled words at the start of the page. because the \n",
    "# single 'c' and the 'indd' is an error assocated with the PDF reader, it is \n",
    "# safe to say that they will be on every page. As such, there are a few ways to\n",
    "# handle this, manually remove them using loops with or without regex. Another\n",
    "# would be to add them to the stopword index in spacy. \n",
    "\n",
    "# here I manually do it, but see my Natural Language Processing workbook in\n",
    "# this directory for more detail on this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thr', 'nancial', 'statements', 'interest', 'expense', 'the', 'amount', 'of', 'interest', 'on', 'debt', 'interest', 'bearing', 'liabilities', 'for', 'the', 'period', 'nancing', 'charges', 'may', 'also', 'be', 'included', 'such', 'as', 'loan', 'origination', 'fees', 'income', 'tax', 'expense', 'the', 'total', 'amount', 'due', 'the', 'government', 'both', 'federal', 'and', 'state', 'on', 'the', 'amount', 'of', 'taxable', 'income', 'of', 'the', 'business', 'during', 'the', 'period', 'taxable', 'income', 'is', 'multiplied', 'by', 'the', 'appropriate', 'tax', 'rates', 'the', 'income', 'tax', 'expense', 'does', 'not', 'include', 'other', 'types', 'of', 'taxes', 'such', 'as', 'unemployment', 'and', 'social', 'security', 'taxes', 'on', 'the', 'company', 'payroll', 'these', 'other', 'nonincome', 'taxes', 'are', 'included', 'in', 'operating', 'expenses', 'indd', 'indd']\n"
     ]
    }
   ],
   "source": [
    "# Here the spacy doc is converted back to a Python list and all the single\n",
    "# chars are removed\n",
    "\n",
    "scrubbed_tokens = [token.text for token in spacy_doc if len(token)!=1]\n",
    "print(scrubbed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['interest', 'expense', 'the', 'amount', 'of', 'interest', 'on', 'debt', 'interest', 'bearing', 'for', 'the', 'period', 'may', 'also', 'be', 'included', 'such', 'as', 'loan', 'origination', 'income', 'tax', 'expense', 'the', 'total', 'amount', 'due', 'the', 'government', 'both', 'federal', 'and', 'state', 'on', 'the', 'amount', 'of', 'taxable', 'income', 'of', 'the', 'business', 'during', 'the', 'period', 'taxable', 'income', 'is', 'by', 'the', 'appropriate', 'tax', 'the', 'income', 'tax', 'expense', 'does', 'not', 'include', 'other', 'of', 'such', 'as', 'unemployment', 'and', 'social', 'security', 'on', 'the', 'company', 'payroll', 'these', 'other', 'are', 'included', 'in', 'operating']\n"
     ]
    }
   ],
   "source": [
    "# Belwo the two misspelled words at the start of the list are removed\n",
    "# along with the 2 'indd' at the end. \n",
    "\n",
    "# pop() could be used here to remove the last two items, and remove() could\n",
    "# be used for the first two mispelled words. However, because these may not\n",
    "# always appear at the start and end, it is a good idea to weed them out\n",
    "\n",
    "# Once again there are a couple of ways to go about this, from quick searches\n",
    "# I found an nltk solution but there is a library called pyenchant that \n",
    "# looked interesting. \n",
    "\n",
    "# Here I use nltk\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "#print('financial' in words.words())\n",
    "\n",
    "# Before removing the words I want to point out that in word count analysis\n",
    "# that the first two words are just incomplete and in reality would actually\n",
    "# need to be included\n",
    "# TODO SEE IF THAT pyenchant library WILL WORK FOR THIS, TO RESPELL THEM?\n",
    "\n",
    "# Here I jsut used the nltk words library which removes them\n",
    "# notice it takes a while to do a word by word checkt his way\n",
    "final_list = [token for token in scrubbed_tokens if token in words.words()]\n",
    "print(final_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In conclusion, the quickest way to remove \"known\" re-occuring unwanted tokens such as 'indd' above, it is best to add them to a list of stopwords, either self-created or with a library like spacy**\n",
    "\n",
    "## TODO, Find and figure out the best way to discover misspelled words so that the first two words pictured below were not removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/pdf_data_almost_clean.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
